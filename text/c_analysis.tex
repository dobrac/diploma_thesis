In this section, I am going to introduce Protocol Buffers.
Then, I will analyze existing solutions for website documentation with the ability to call APIs for Protocol Buffers, GraphQL, and RESTful APIs.
I will start with Protocol Buffers, then move and explore the solutions to GraphQL, and finally analyze RESTful API\@.


\section{Protocol Buffers}
Protocol Buffers are a mechanism for serializing structured data.
They are language-neutral and platform-neutral.
They encompass:
\begin{itemize}
    \item definition language,
    \item compiler-generated code,
    \item language-specific runtime libraries,
    \item serialization format.
\end{itemize}
The definition language defines data structures expressed in .proto files.
The compiler-generated code enables interaction with the defined data structures in a specific programming language.
The language-specific runtime libraries facilitate the serialization and deserialization of data according to the Protocol Buffer format.
The serialization format is a compact binary format for storing Protocol Buffer data in files or transmitting it across network connections.
\cite{protobuf-overview}

The mechanism of typical workflow is described in figure~\ref{fig:protobuf-mechanism}.
For this work, my main focus will be generating the code (and website documentation) from the .proto files and using the generated code to interact with the final APIs.
\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/protocol-buffers-concepts.png}
    \caption{Protocol Buffers Workflow~\cite{protobuf-overview}}
    \label{fig:protobuf-mechanism}
\end{figure}

There are two language versions of Protocol Buffers, version 2 and version 3.
The versions share the same basic concepts using the same syntax, but version 3 improves version 2 in several ways~\cite{protobuf-proto3}.
As this work focuses on the latest technologies, I will focus on version 3 of Protocol Buffers.
If no specific version is mentioned, it is assumed that version 3 is used.

\subsection{Structures}
The primary keywords in Protocol Buffers are \textit{message}, \textit{enum}, \textit{service}, \textit{method}, and \textit{package}.
The \textit{message} is used to define a data structure.
The \textit{enum} defines a set of named constants.
The \textit{service} is a set of \textit{methods} that can be called remotely.
And, the \textit{package} type is used to define a namespace for the defined \textit{messages}, \textit{enums}, and \textit{services}.
\cite{protobuf-proto3}

\subsubsection{Message}
Messages are used to define data structures.
They are defined using the \textit{message} keyword followed by the name of the message and a block of fields.
Each field has a name, a type, and a unique number across all fields in the \textit{message}.
The type of a field can be a scalar type or another \textit{message} type.
The possible scalar types are described in table~\ref{tab:protobuf-scalar-types} with their C++ counterparts.
The unique number is used to identify the field in the binary encoding.
Reusing the same number for different fields is therefore highly discouraged.
To avoid this, there is a \textit{reserved} keyword, which I will describe later.
\cite{protobuf-proto3}

\begin{table}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{|l|l|}
        \hline
        .proto   & C++    \\ \hline
        double   & double \\ \hline
        float    & float  \\ \hline
        int32    & int32  \\ \hline
        int64    & int64  \\ \hline
        uint32   & uint32 \\ \hline
        uint64   & uint64 \\ \hline
        sint32   & int32  \\ \hline
        sint64   & int64  \\ \hline
        fixed32  & uint32 \\ \hline
        fixed64  & uint64 \\ \hline
        sfixed32 & int32  \\ \hline
        sfixed64 & int64  \\ \hline
        bool     & bool   \\ \hline
        string   & string \\ \hline
        bytes    & string \\ \hline
    \end{tabular}
    \caption{Scalar Types in Protocol Buffers~\cite{protobuf-proto3}}
    \label{tab:protobuf-scalar-types}
\end{table}

% Numbering
% Repeated
% Map
% Optional
% One Of
% Options
% reserved

Additional properties can be added to fields or types to alter their behavior.
The possibilities are \textit{reserved}, \textit{optional}, \textit{repeated}, \textit{map}, and \textit{oneof}.
\cite{protobuf-proto3}

The \textit{reserved} keyword is used to reserve a field number, preventing it from being used in the future.
This is useful when a field is removed from a message, and the field number should not be reused.
You can specify a single field number, a range of field numbers, or a list of field numbers and ranges.
\cite{protobuf-proto3}

In Protocol Buffers version 3, fields are inherently optional, with omitted fields assuming their default values.
This can create ambiguity when differentiating between a missing field and one explicitly set to its default.
The \textit{optional} keyword resolves this by providing a mechanism to explicitly mark fields as optional and track whether they have been set, even if the value is the default.
\cite{protobuf-proto3}

The \textit{repeated} keyword is used to define fields that can hold multiple values of the same type.
This is analogous to arrays or lists in common programming languages.
A repeated field allows you to represent collections of data within your message structure.
For example, a \textit{message} representing an order might have a \textit{repeated} field for line items, allowing multiple products within a single order.
Protocol Buffers offer efficient encoding mechanisms for repeated fields, making them suitable for representing ordered data lists.
\cite{protobuf-proto3}

The \textit{map} keyword is employed to define fields encompassing key-value pairs, akin to dictionaries or hashmaps in programming contexts.
A \textit{map} field allows the flexible association of related data without the constraints of a rigid structure.
For instance, a product attribute message could leverage a \textit{map} field where keys denote attribute names (``color'', ``size'') and their corresponding values provide the descriptions (``red'', ``large'').
\cite{protobuf-proto3}

Finally, the \textit{oneof} keyword provides a mechanism to define a message field where only one of several sub-fields can be set at a time.
This is valuable when the \textit{message} needs to represent mutually exclusive data variations.
For example, a payment\_method field within a \textit{message} could use a \textit{oneof} to support different payment types like credit\_card, debit\_card, or PayPal.
Setting one of these sub-fields automatically clears any previously set values within the \textit{oneof}.
This helps conserve memory and enforces a clear structure for alternative data representations.
\cite{protobuf-proto3}

\subsubsection{Enum}
Another type of structure is \textit{enum}.
It is used to define a set of named constants.
The constants are defined using the \textit{enum} keyword followed by the name of the \textit{enum} and a block of constants with their numeric values.
The special numeric value 0 is used as the default value.
Therefore, the first constant must have the value 0.
\cite{protobuf-proto3}

The \textit{enum} has a special option called \textit{allow\_alias}, which allows having the same numeric values for multiple names.
This is useful when the same value is used in different contexts.
\cite{protobuf-proto3}

\subsubsection{Service and Method}
The \textit{service} defines a set of \textit{methods} that can be called remotely.
It is defined using the \textit{service} keyword followed by the name of the \textit{service} and a block of \textit{methods}.
Each \textit{method} has a name, request, and response \textit{message} type.
The \textit{method} can also have a \textit{stream} keyword to define streaming of request, response, or both.
\cite{protobuf-proto3}

Streaming is a feature that allows the client and server to send a sequence of \textit{messages} back and forth until the stream is closed.
This is useful when the client or server needs to send a large number of \textit{messages} and does not know the exact number of \textit{messages} in advance.
\cite{protobuf-proto3}

There are four types of gRPC calls.
When no streaming is involved, it is called \textit{unary} call.
When the request is streamed, it is called \textit{client-streaming} call.
When the response is streamed, it is called \textit{server-streaming} call.
And when both request and response are streamed, it is called \textit{bidirectional-streaming} call.
\cite{grpc-core-concept}

\subsubsection{Packages}
The \textit{package} is used to define a namespace for the defined \textit{messages}, \textit{enums}, and \textit{services} in the .proto file.
It is present at the beginning of the file using the \textit{package} keyword followed by the name of the \textit{package}.
The \textit{package} may not be considered for the code generation.
This is especially true for Java.
Because of that, a \textit{java\_package} option can be used to define the package for the generated code.
This option is defined at the file level.
\cite{protobuf-proto3}

The other option to differentiate \textit{messages} names is using nested types.
The \textit{message} can be defined inside another \textit{message}.
This is useful when the \textit{message} is used only in the context of the parent \textit{message} or is meaningful only in the parent \textit{message} context.
\cite{protobuf-proto3}

Both \textit{package} and nested types are used to avoid name conflicts and can be used using the dot notation between names.

\subsection{Comments}
The .proto files can contain comments.
The comments can be single-line or multi-line.
The single-line comments are started with the \textit{//} characters.
The multi-line comments start with the \textit{/*} characters and end with the \textit{*/} characters.
The comments can be used to describe the purpose of the \textit{message}, \textit{enum}, \textit{service}, \textit{method} or \textit{field}.
The comments can also describe the purpose of the \textit{package} or the whole file.
\cite{protobuf-proto3}

\subsection{Code Generation}
The .proto files generate the code for the specific programming language.
Code generation can be done using various tools, the recommended one being the Protocol Compiler (protoc).
It is used to generate the C++, C\#, Dart, Go, Java, Python, Ruby, and JavaScript code.
An example usage is described in the code snippet~\ref{lst:protobuf-protoc}.
Required classes and types are then generated, and the gRPC APIs can be called without extra coding work.
\cite{protobuf-proto3}

\begin{lstlisting}[language=bash, caption={Protocol Buffers Code Generation~\cite{protobuf-proto3}}, label={lst:protobuf-protoc}]
protoc --proto_path=IMPORT_PATH --java_out=DST_DIR path/to/file.proto
\end{lstlisting}

\subsection{Metadata}
Metadata are key-value pairs sent with initial or final request or response.
They are used to provide additional information, such as authentication or tracing information.
Two types of metadata are used, headers and trailers.

Headers are sent before the initial client request and before the initial response from the server.
This applies only for the first message of the client and servery.
The figure~\ref{fig:grpc-metadata} shows the gRPC headers in the request lifecycle.

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.4\textwidth]{images/grpc-metadata}
    \caption{gRPC Metadata}
    \label{fig:grpc-metadata}
\end{figure}

Trailers are sent after the final response from the server.
They are used to provide additional information about the response, such as the utilization or query cost.
The figure~\ref{fig:grpc-metadata} shows the gRPC trailers in the response lifecycle.

\cite{grpc-metadata}

\subsection{gRPC-web}
The gRPC is built on HTTP/2, using features like HTTP/2 framing~\cite{grpc-protocol-http2}.
As the HTTP/2 framing is not, and probably never be, directly exposed by any browser, the gRPC-Web protocol exists~\cite{grpc-protocol-web}.

The design goals of the gRPC-Web are:
\begin{itemize}
    \item to adopt the same framing as gRPC whenever possible,
    \item decouple from HTTP/2 framing,
    \item support text stream for cross-browser support~\cite{grpc-protocol-web}.
\end{itemize}

The gRPC-Web is adding a proxy between the server and the browser.
The communication works as follows.
The browser sends a request to the proxy using the gRPC-Web protocol.
The proxy translates the gRPC-Web protocol to the gRPC protocol and sends the request to the server.
The server processes the request and sends the response back to the proxy.
The proxy translates the gRPC protocol to the gRPC-Web protocol and sends the response back to the browser.
The browser processes the response, and the communication is complete.
\cite{grpc-protocol-web}

The default proxy implementation is the Envoy\footnote{\url{https://www.envoyproxy.io/}} proxy.
It supports the gRPC-Web protocol out of the box.
Other options are, but not only, gRPC-web Go proxy\footnote{\url{https://github.com/improbable-eng/grpc-web/tree/master/go/grpcwebproxy}}, APISIX\footnote{\url{https://apisix.apache.org/blog/2022/01/25/apisix-grpc-web-integration/}}, and Nginx\footnote{\url{https://www.nginx.com/}}.

Because of the proxy and browser implementation, there are a few differences and current limitations of the gRPC-Web~\cite{grpc-web}.
The most important one is streaming support.
Currently, the gRPC-Web does not support client-side streaming (and effectively bi-directional streaming).
It supports only unary calls and server-side streaming.
Based on the streaming roadmap, the client-side streaming is planned for the future.
It is planned for 2023+, but it has not been implemented yet~\cite{grpc-web-streaming-roadmap}.

\subsection{gRPC Reflection}
The gRPC protocol uses binary encoding.
Therefore, it is impossible to query the server without knowing the protobuf definition of the service and both request and response messages before the request.
The gRPC reflection is a way to get this information using a standardized gRPC service that allows other clients to query the server for the protobuf-defined APIs.
It includes all necessary information about the services, methods, enums, and messages.
This information can encode requests, query the server, and decode responses.
It is used by debugging tools such as grpcurl\footnote{\url{https://github.com/fullstorydev/grpcurl}}.
The gRPC reflection service is not exposed by default, so it must be explicitly enabled in the server configuration.
The support for it varies across different gRPC implementations in different programming languages.
\cite{grpc-reflection}


\section{Documentation Websites}
This section examines popular tools for documenting gRPC, GraphQL, and RESTful APIs.
I'll assess the capabilities and shortcomings of each solution, addressing gRPC first, then GraphQL, and lastly RESTful APIs.
The section will culminate in a summary of findings, a discussion of issues, and a proposed solution for the static web generator.

\subsection{Protocol Buffers}
In the Protocol Buffers world, there are several tools for generating documentation or interactive call from the .proto files.
The most popular ones, that I have found, are described in the following subsections.
% TODO: Write something more here

\subsubsection{Wombat}
Wombat is a cross platform gRPC desktop app client.
The UI is in the figure~\ref{fig:grpc-wombat}.
It is used to call gRPC services and inspect the responses.
\cite{grpc-wombat}

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/grpc/wombat}
    \caption{Wombat GUI~\cite{grpc-wombat}}
    \label{fig:grpc-wombat}
\end{figure}

The main features are:
\begin{itemize}
    \item automatic parsing of proto definitions to render services and input messages,
    \item configuration of TLS,
    \item input form generation for all scalar types, nested messages, enums, repeated, oneof and map,
    \item request metadata,
    \item execution of unary, server streaming, client streaming, bidirectional requests,
    \item pending request cancellation,
    \item sending EOF for client streaming,
    \item view response messages,
    \item view gRPC header and trailer,
    \item view RPC statistics,
    \item determine gRPC schema via reflection,
    \item support for Google Well Known Types,
    \item multiple workspace support~\cite{grpc-wombat}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item desktop application (not a website),
    \item no support for documentation comments,
    \item last update 3 years ago~\cite{grpc-wombat}.
\end{itemize}

% Bugs: it doesn't remember the responses when you switch methods, UI is unfamiliar, lot of errors even if you work with the software in valid way

\subsubsection{BloomRPC}
BloomRPC is a cross platform gRPC desktop app client.
The UI is in the figure~\ref{fig:grpc-bloomrpc}.
It is used to call gRPC services and inspect the responses.
\cite{grpc-bloomrpc}

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/grpc/bloomrpc}
    \caption{BloomRPC GUI~\cite{grpc-bloomrpc}}
    \label{fig:grpc-bloomrpc}
\end{figure}

The main features are:
\begin{itemize}
    \item automatic parsing of proto definitions to list services and example messages,
    \item configuration of TLS,
    \item request metadata,
    \item execution of unary, server streaming, client streaming, bidirectional requests,
    \item selection between gRPC and gRPC-Web,
    \item pending request cancellation,
    \item sending EOF for client streaming,
    \item view response messages~\cite{grpc-bloomrpc}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item desktop application (not a website),
    \item no support for documentation comments,
    \item missing determine gRPC schema via reflection,
    \item missing gRPC headers and trailers preview,
    \item archived in 2023, usage is no longer recommended~\cite{grpc-bloomrpc}.
\end{itemize}

\subsubsection{gRPC Docs and GenDocu}
The gRPC Docs is a website API documentation generator by GenDocu.
It provides RPC calls documentation for gRPC services.
There is no option of calling the services.
This documentation is generated from the .proto files using protoc-gen-doc\footnote{\url{https://github.com/pseudomuto/protoc-gen-doc}} utility with it's custom format output as JSON\@.
The example web UI is in the figure~\ref{fig:grpc-gendocu}.
\cite{grpc-gendocu}

GenDocu is a hosted gRPC Docs version.
This version is able to call the gRPC services.
But, at the time of writing, the GenDocu website is not working anymore and the project looks to be abandoned (with the last commit more than a year ago).
\cite{grpc-gendocu}

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/grpc/gendocu}
    \caption{GenDocu Web UI~\cite{grpc-gendocu}}
    \label{fig:grpc-gendocu}
\end{figure}

The main features are:
\begin{itemize}
    \item .proto files parsing and services, messages, enums generation,
    \item support for documentation comments,
    \item generation using common JSON format with ability to change the source without redeploying the website~\cite{grpc-gendocu}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item missing determine gRPC schema via reflection,
    \item execution of unary, server streaming, client streaming, bidirectional requests,
    \item inactive and looks to be abandoned with no external websites working~\cite{grpc-gendocu}.
\end{itemize}

\subsubsection{gRPC UI}
The gRPC UI is a website with support of calling gRPC services.
With this tool you can browse the schema, which is presented as a list of available endpoints.
The schema can be constructed by querying a server that supports server reflection, by reading proto source files, or by loading a compiled `protoset' file (files that contain encoded file descriptor protos).
The protoset file can be created using the protoc tool, used by the gRPC tooling for client's code generation.
The UI is in the figure~\ref{fig:grpc-grpcui}.
\cite{grpc-grpcui}

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/grpc/grpcui}
    \caption{gRPC UI~\cite{grpc-grpcui}}
    \label{fig:grpc-grpcui}
\end{figure}

The main features are:
\begin{itemize}
    \item listing services and methods,
    \item ability to construct messages with forms or raw JSON,
    \item execution of unary, server streaming, client streaming, bidirectional requests,
    \item request metadata, headers and trailers,
    \item configuration of TLS,
    \item rich support for well-known types,
    \item view response messages,
    \item actively maintained~\cite{grpc-grpcui}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item requires a server to run (not a static website),
    \item it requires you to construct the entire stream of request messages all at once and then it shows the entire resulting stream of response messages all at once (so you can't interact with any streams interactively)~\cite{grpc-grpcui}.
\end{itemize}

\subsubsection{letmegrpc}
The gRPC UI is a website with support of calling gRPC services.
It allows service methods calling using forms, where each method has it's separate url (\url{http://localhost:8080/ServiceName/MethodName}).
It is constructed from .proto file.
The UI is in the figure~\ref{fig:grpc-letmegrpc}.
\cite{grpc-letmegrpc}

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/grpc/letmegrpc}
    \caption{letmegrpc UI~\cite{grpc-letmegrpc}}
    \label{fig:grpc-letmegrpc}
\end{figure}

The main features are:
\begin{itemize}
    \item comments to fields are in tooltips,
    \item ability to construct messages with forms,
    \item execution of unary, server streaming, client streaming, bidirectional requests,
    \item view response messages~\cite{grpc-letmegrpc}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item requires setup with manual go package downloads,
    \item no listing of services and methods,
    \item requires a server to run (not a static website),
    \item even though forms are powerful, they can be annoying to use, especially with complex requests,
    \item no support for request metadata, headers and trailers,
    \item no direct support for gRPC reflection,
    % TODO: Correct citation?
    \item custom proto file parser, which does not have implemented all language features (\cite{grpc-letmegrpc-issue44}),
    \item inactive, last update 5 years ago~\cite{grpc-letmegrpc}.
\end{itemize}

\subsubsection{gRPC-swagger}
The gRPC Swagger is a website.
It is based on gRPC reflection and can be used to list and call gRPC services.
It is using Swagger UI design language (copy of the Swagger UI), but it is not a part of the official Swagger.
The UI is in the figure~\ref{fig:grpc-swagger}.
The architecture is done using it's own server as a proxy.
Website's server forwards all requests to the gRPC server and then back to the website.
\cite{grpc-letmegrpc}

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/grpc/swagger-1}
    \includegraphics[width=0.8\textwidth]{images/grpc/swagger-2}
    \caption{gRPC Swagger UI~\cite{grpc-swagger}}
    \label{fig:grpc-swagger}
\end{figure}

The main features are:
\begin{itemize}
    \item listing of services and methods,
    \item familiar UI to the Swagger UI,
    \item support for request metadata, headers and trailers,
    \item execution of unary, server streaming, client streaming, bidirectional requests,
    \item view response messages~\cite{grpc-swagger}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item requires reflection enabled,
    \item no support for comments,
    \item requires a server to run (not a static website),
    \item it requires to construct the entire stream of request messages all at once and then it shows the entire resulting stream of response messages all at once (so you can't interact with any streams interactively)
    \item request data are only in the form of JSON,
    \item inactive, last update 2 years ago, last release in 2020~\cite{grpc-swagger}.
\end{itemize}

\subsubsection{gRPC-Gateway}
The gRPC-Gateway is a protoc compiler plugin.
It generates a reverse-proxy server which translates a RESTful JSON API into gRPC\@.
So, it is a reverse proxy server and not a documentation website, nor a client for calling gRPC\@.
It can also generate OpenAPI definitions using protoc-gen-openapiv2, which can be then used with tools like Swagger UI\@.
\cite{grpc-letmegrpc}

It is using annotations in the service definitions or a configuration file.
An example of annotation is in the code snippet~\ref{lst:grpc-gateway-annotations},
and an example of configuration file is in the code snippet~\ref{lst:grpc-gateway-configuration}.
\cite{grpc-letmegrpc}


\begin{lstlisting}[language=protobuf2, style=protobuf, caption={gRPC-Gateway Annotations~\cite{grpc-gateway}}, label={lst:grpc-gateway-annotations}]
import "google/api/annotations.proto";

rpc Echo(StringMessage) returns (StringMessage) {
  option (google.api.http) = {
    post: "/v1/example/echo"
    body: "*"
  };
}
\end{lstlisting}

\begin{lstlisting}[style=yaml, caption={gRPC-Gateway Configuration File~\cite{grpc-gateway}}, label={lst:grpc-gateway-configuration}]
type: google.api.Service
config_version: 3

http:
  rules:
    - selector: your.service.v1.YourService.Echo
      post: /v1/example/echo
      body: "*"
\end{lstlisting}

The main features are:
\begin{itemize}
    \item support for request metadata, headers,
    \item ability to create OpenAPI definitions,
    \item execution of unary, server streaming, client streaming, bidirectional requests, but in a batched manner,
    \item and lot more regards the specifics of the reverse-proxy layer~\cite{grpc-gateway}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item dependent on OpenAPI/HTTP API - is not just a gRPC client, but much more,
    \item OpenAPI definitions do not have to reflect the gRPC service definitions,
    \item no support for trailers,
    \item no support for true bi-directional streaming,
    \item requires the underline gRPC service definitions,
    \item no support for reflection,
    \item no support for documentation/comments,
    \item requires a server to run (not a static website)~\cite{grpc-gateway}.
\end{itemize}

\subsubsection{Postman}
Postman is one of the most popular application for API calling.
It supports lot of different types of APIs (e.g.\ HTTP API, WebSockets, GraphQL), but including also gRPC\@.
Upon loading .proto files or using gRPC reflection, it can understand the gRPC API and use the information for calls.
It can do much more than just calling the gRPC services, but it is not a website for documentation, so I will focus on it's gRPC capabilities.
The UI is in the figure~\ref{fig:postman}.
\cite{postman}

\begin{figure}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.8\textwidth]{images/postman}
    \caption{Postman UI~\cite{postman}}
    \label{fig:postman}
\end{figure}

The main features are:
\begin{itemize}
    \item listing of services and methods,
    \item execution of unary, server streaming, client streaming, bidirectional requests,
    \item broad range of features for API calling, including automating tests,
    \item support for request metadata, headers and trailers,
    \item supports gRPC reflection,
    \item messages autocompletion,
    \item messages validation,
    \item view response messages with full support for streaming~\cite{postman}.
\end{itemize}

The main disadvantages (compared to my task) are:
\begin{itemize}
    \item it is an application, not a static website,
    \item no support for comments,
    \item request data are only in the form of JSON~\cite{postman}.
\end{itemize}

\subsubsection{proto2asciidoc}
The proto2asciidoc is a plugin for protoc tool that generates AsciiDoc documentation.
The AsciiDoc format can be then parsed by many tools.
One of these tools can be a static website.
The main disadvantage is that it is not a website itself or application, but only the documentation file.
Therefore, it does not allow calling the gRPC services at all.
Another disadvantage is a requirement of special comment blocks formatting, which is different from usual comments in proto files.
And finally, it looks to be abandoned with the last commit more than 2 years ago.
\cite{grpc-proto2asciidoc}

\subsubsection{protoc-gen-doc}
A documentation generator for protoc compiler tool.
It can generate HTML, JSON, DocBook, and Markdown documentation from comments in .proto files.
The main advantages are that it can take comments from the .proto files and generate a static documentation website or JSON, keeping the original structure of the input .proto files.
The website contains the documentation of the services, methods, and messages.
The JSON can be then used to create a custom website, for example GenDocu mentioned earlier uses it.
The disadvantage is that it does not allow calling the gRPC services at all.
\cite{grpc-protoc-gen-doc}

% Example website: https://rawgit.com/pseudomuto/protoc-gen-doc/master/examples/doc/example.html

\subsection{GraphQL}

\subsubsection{graphdoc}

\subsubsection{graphql-playground}

\subsubsection{graphiql}

\subsection{RESTful API}

\subsubsection{Swagger UI}

\subsection{Comparison and Summary}

\subsubsection{Issues}


\section{Requirements}
Based on the problems and primary requirements described previously, I have compiled functional and non-functional requirements covering the required functionality of the static web generator.

\subsection{Functional}
% Co má systém umět

%\subsubsection{F1 -- Pomocník pro inicializaci obrazu softwaru Syllabus Plus}
%Pro inicializaci obrazu softwaru Syllabus Plus a další práci s daty je potřeba zadat informace o daném semestru.
%Správné nastavení je klíčové pro korektní plánování časových lístků.

\subsection{Non-Functional}

%\subsubsection{N1 -- Aplikace s grafickým uživatelským rozhraním}
%Aplikace by měla mít grafické uživatelské rozhraní pro práci s daty.
%Uživatel by tak měl být schopen obsluhovat aplikaci sám bez pomoci další osoby.


\section{Use Cases}


%\begin{figure}
%    \centering
%    \captionsetup{justification=centering}
%    \includegraphics[width=1.0\textwidth]{use-case-diagram}
%    \caption{Diagram případů užití}
%    \label{fig:use-case-diagram}
%\end{figure}

%\subsection{UC1 -- Inicializace obrazu softwaru Syllabus Plus}
%Rozvrhář spustí software Syllabus Plus a začne s inicializací nového obrazu.
%Během inicializace se mu zobrazí dialog pro nastavení začátku semestru, vyučované dny, začátek a konec hodin, počet hodin za den a počet týdnů na semestr.
%Rozvrhář otevře aplikaci pro převod a nastaví semestr, který se využije pro načítání dat ze systému KOS\@.
%Zobrazí si jakým způsobem má položky nastavit, aby byla data správně reprezentována (například při plánování časových lístků).
%Formulář vyplní podle získaných dat z aplikace a inicializuje obraz.
%V případě, že se rozvrhář splete, musí aktuální obraz smazat a začít vytvářet nový od začátku.
%Jinak je inicializace úspěšně dokončena.


\section{Requirements to Use Cases Mapping}
I have compiled a table of requirements and use cases mapping (see table~\ref{tab:use_cases}) to verify that all requirements are covered by at least one use case and that no use case is unnecessary.
The table shows that all requirements are covered.

\begin{table}[hbt!]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
        \hline
        & \multicolumn{15}{c|}{Use Cases} \\ \hline
        & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\ \hline
        F1  & x &   &   &   &   &   &   &   &   &    &    &    &    &    &    \\ \hline
        F2  &   &   & x &   &   &   &   &   &   &    &    &    &    &    &    \\ \hline
        F3  &   &   &   & x &   &   &   &   &   &    &    &    &    &    &    \\ \hline
        F4  &   &   &   &   &   &   &   &   &   &    &    &    &    &    & x  \\ \hline
        F5  &   &   &   &   & x &   &   &   &   &    &    &    &    &    &    \\ \hline
        F6  &   & x &   &   &   &   &   & x &   &    &    & x  & x  &    &    \\ \hline
        F7  &   & x &   &   &   &   &   &   &   &    &    & x  &    &    &    \\ \hline
        F8  &   &   &   &   &   &   &   &   &   &    &    &    & x  &    &    \\ \hline
        F9  &   & x &   &   &   &   &   &   &   &    & x  &    &    &    &    \\ \hline
        F10 &   & x &   &   &   &   &   &   &   &    &    &    &    &    &    \\ \hline
        F11 &   &   &   &   &   &   &   &   & x & x  &    &    &    &    &    \\ \hline
        F12 &   & x &   &   &   &   &   &   &   &    &    &    &    &    &    \\ \hline
        F13 &   &   &   &   &   &   &   &   &   & x  &    &    &    &    &    \\ \hline
        F14 &   &   &   &   &   &   &   & x & x & x  &    & x  & x  &    &    \\ \hline
        F15 &   &   &   &   &   &   &   & x & x & x  &    & x  &    &    &    \\ \hline
        F16 &   &   &   &   &   &   &   & x & x & x  &    & x  & x  &    &    \\ \hline
        F17 &   &   &   &   &   & x & x &   &   &    &    &    &    &    &    \\ \hline
        F18 &   &   &   &   &   & x &   &   &   & x  &    &    &    & x  &    \\ \hline
        F19 & x &   &   &   &   &   &   &   &   &    &    &    &    &    &    \\ \hline
    \end{tabular}
    \caption{Requirements to Use Cases Mapping}
    \label{tab:use_cases}
\end{table}